{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Recommendation Testing\n",
    "Validating a recommender system is no trivial task. ECommerce companies usually trials them extensively using both online and offline sources. As this is not quite feasible in our case, we resolved to using a different approach: As discussed when training the Item2Vec models, we created on distinct model for each customer cluster. Next, we load the test users that we defined. We do make some assumptions when actually making the recommendations:\n",
    "\n",
    "1. The (true) embedding of the basket can be approximated by a (softmax-weighted) average of the items contained in that basket.\n",
    "2. As in training the Item2Vec models, we only consider orders that have at least 4 items in them as we believe that just having one item in the basket and predicting the remaining one is incredibly difficult.\n",
    "\n",
    "We then proceeded to generate \"artificial\" test datasets by using a rolling convolution to extract \"order windows\" of the following shape (e.g. by using a filter of size 4x1):\n",
    "\n",
    "([Item1, Item2, Item3, Item4, Item5]) : \n",
    " \n",
    "Convolution 1: [Item1, Item2, Item3, Item4]  \n",
    "Convolution 2: [Item2, Item3, Item4, Item5]  \n",
    "\n",
    "We then extract the first three basket elements as our \"basket\" (e.g. [Item1, Item2, Item3]), apply the recommender on these items and then compare with the last item  [Item4] in the convolution. \n",
    "\n",
    "We made an important observation when using the recommender here: Using a softmax-weighted basket (i.e. weighting the first element with $e^1$, the second with $e^2$ and then normalizing by the sum of $e^1$ to $e^3$) yields a superior result compared to using the simple mean. This is intuitive, as products that have been purchased at the beginning might not be so indicative of products further ahead in the cart.\n",
    "\n",
    "Overall we find that the recommender performs reasonably well against a random benchmark. By simply using a random sample from the 60,000 products or so in Instacarts database, we would expected to be picking the right product with a chance of 1/59,999. Depending on the cluster used, we achieve accuracies as high as 9% (which might be a statistical fluke) and and as low as 0.9%, both of which are a significant improvement over a simple guess. We see that the overall error variance is quite pronounced between the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import chain\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from typing import List\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_item_models = [Word2Vec.load(f\"model_cluster_{id}.model\") for id in range(0, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('product_lookup.pkl', 'rb') as file:\n",
    "    product_lookup = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_users(id: int):\n",
    "    with open(f\"test_users_cluster{id}.pkl\", \"rb\") as file:\n",
    "        test_users = pickle.load(file)\n",
    "    return test_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = [load_test_users(id) for id in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(data_dir: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    data_dir: str\n",
    "      The path where the data is stored\n",
    "\n",
    "    Returns:\n",
    "    ----------------\n",
    "    dataframes_ls: List[pd.DataFrame]\n",
    "      A list of pandas dataframes\n",
    "    \"\"\"\n",
    "    files = [file.split('.')[0] for file in listdir(data_dir) if file.split('.')[0] != \"\"]\n",
    "\n",
    "    # Creating a string expression to evaluate the data\n",
    "    eval_expr = ', '.join(f'pd.read_csv(\\'{data_dir}/{file}.csv\\')' for file in files)\n",
    "\n",
    "    # Evaluating the expression and assigning it, which creates a list of dataframes\n",
    "    dataframes_ls = eval(eval_expr)\n",
    "\n",
    "    return dataframes_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('products.csv')\n",
    "cluster_data = pq.read_table('./dummy_k5.parquet').to_pandas()\n",
    "cluster_data_named = pd.merge(cluster_data, products, on='product_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data_named['product_id'] = cluster_data_named['product_id'].astype(str)\n",
    "cluster_data_named['user_id'] = cluster_data_named['user_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_cluster(data: pd.DataFrame, cluster_num: int):\n",
    "    return data.loc[data['cluster'] == cluster_num, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_separated = [filter_data_by_cluster(cluster_data_named, cluster_num) for cluster_num in range(0, len(cluster_data_named['cluster'].unique()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_cluster(cluster: pd.DataFrame, users):\n",
    "    return cluster[cluster['user_id'].isin(users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orders_from_cluster(cluster_subset):\n",
    "    return cluster_subset.groupby(['user_id', 'order_id'])['product_id'].apply(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_purchase_history_in_cluster(cluster: pd.DataFrame, users):\n",
    "    cluster_subset = subset_cluster(cluster, users)\n",
    "    purchase_history = get_orders_from_cluster(cluster_subset)\n",
    "    filtered_purchase_history = [purchase for purchase in purchase_history if len(purchase) > 3] # A number of purchases \n",
    "    return purchase_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_history_validation = [generate_user_purchase_history_in_cluster(clusters_separated[i], test_users[i]) for i in range(0, len(clusters_separated))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, the product recommender retrieves the k most similar items for the averaged item vectors in the basket and checks whether one of the recommended products is indeed the next item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_product(cluster_model, product_lookup, product_ids):\n",
    "\n",
    "    def filter_matches(cluster_model, product_ids):\n",
    "        return [product_id for product_id in product_ids if cluster_model.wv.__contains__(product_id)]\n",
    "\n",
    "    filtered_matches = filter_matches(cluster_model, product_ids)\n",
    "\n",
    "    if len(filtered_matches) == 0:\n",
    "        return 'UNKNOWN' # Returning an \"UNKNOWN\" token for an empty basket\n",
    "    else:\n",
    "        def average_item_vectors(cluster_model, product_ids):\n",
    "            embeddings = [cluster_model.wv[product_id] for product_id in product_ids]\n",
    "            def softmax_weights(embeddings):\n",
    "                raw_weights = [np.exp(i) for i in range(1, len(embeddings)+1)]\n",
    "                softmax_weights = np.array([raw_weight/sum(raw_weights) for raw_weight in raw_weights])\n",
    "                return softmax_weights\n",
    "            sm_weights = softmax_weights(embeddings)\n",
    "            return np.average(embeddings, axis=0, weights=sm_weights)\n",
    "            \n",
    "        basket_vector = average_item_vectors(cluster_model, filtered_matches)\n",
    "\n",
    "        def retrieve_most_similar_products(cluster_model, product_lookup, basket_vector):\n",
    "            similar_products = cluster_model.wv.similar_by_vector(basket_vector, topn=15)[1:]\n",
    "            recommendations = [similar[0] for similar in similar_products]\n",
    "                \n",
    "            return recommendations   \n",
    "\n",
    "        recommendations = retrieve_most_similar_products(cluster_model, product_lookup, basket_vector)\n",
    "\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_prediction_filter(history: np.array, filter_shape: np.array):\n",
    "        history_expanded = np.expand_dims(history, axis=1)\n",
    "        masks = sliding_window_view(history_expanded, filter_shape)\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_recommendations(cluster_model, mask: np.array, product_lookup: dict):\n",
    "    basket = mask.flatten()[:-1]\n",
    "    target_item = mask.flatten()[-1]\n",
    "    recommendations = recommend_product(cluster_model, product_lookup, basket)\n",
    "\n",
    "    if target_item in set(recommendations):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_cluster_model(cluster_model, product_lookup, cluster_history):\n",
    "    validation_history = [np.array(history) for history in cluster_history if len(history) > 3]\n",
    "    masks = [convolve_prediction_filter(history, (4,1)) for history in validation_history]\n",
    "    chained_masks = list(chain.from_iterable(masks))\n",
    "\n",
    "    order_score = sum([validate_recommendations(cluster_model, mask, product_lookup) for mask in masks])/len(masks)\n",
    "    return order_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [score_cluster_model(cluster_item_models[i], product_lookup, purchase_history_validation[i]) for i in range(len(cluster_item_models))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cluster 1', 0.0408),\n",
       " ('Cluster 2', 0.0433),\n",
       " ('Cluster 3', 0.0034),\n",
       " ('Cluster 4', 0.0533),\n",
       " ('Cluster 5', 0.0413)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(f\"Cluster {i+1}\", round(results[i], 4)) for i in range(len(results))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
