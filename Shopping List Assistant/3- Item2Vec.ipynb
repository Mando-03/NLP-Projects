{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Item2Vec \n",
    "Our recommender model is based on the Item2Vec specification, which is a direct transfer from the Word2Vec model first reported by Mikolov et al. (2013) at Google. Instead of using documents with Natural Language, we are using the individual orders of different customers as identified by the KMeans clustering. In this notebook, we are going to train 13 different Item2Vec models, one for each cluster that we have identified. We are going to restrict the data in that we only consider orders with at least 4 items in them, as this should give them recommender sufficient information, apart from the cluster the user belongs to, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from os import listdir\n",
    "from typing import List\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(data_dir: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    data_dir: str\n",
    "      The path where the data is stored\n",
    "\n",
    "    Returns:\n",
    "    ----------------\n",
    "    dataframes_ls: List[pd.DataFrame]\n",
    "      A list of pandas dataframes\n",
    "    \"\"\"\n",
    "    files = [file.split('.')[0] for file in listdir(data_dir) if file.split('.')[0] != \"\"]\n",
    "\n",
    "    # Creating a string expression to evaluate the data\n",
    "    eval_expr = ', '.join(f'pd.read_csv(\\'{data_dir}/{file}.csv\\')' for file in files)\n",
    "\n",
    "    # Evaluating the expression and assigning it, which creates a list of dataframes\n",
    "    dataframes_ls = eval(eval_expr)\n",
    "\n",
    "    return dataframes_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('products.csv')\n",
    "cluster_data = pq.read_table('./dummy_k5.parquet').to_pandas()\n",
    "cluster_data_named = pd.merge(cluster_data, products, on='product_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data_named['product_id'] = cluster_data_named['product_id'].astype(str)\n",
    "cluster_data_named['user_id'] = cluster_data_named['user_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_cluster(data: pd.DataFrame, cluster_num: int):\n",
    "    return data.loc[data['cluster'] == cluster_num, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_separated = [filter_data_by_cluster(cluster_data_named, cluster_num) for cluster_num in range(0, len(cluster_data_named['cluster'].unique()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_users_in_cluster(cluster_data: pd.DataFrame, train_rate: float):\n",
    "    unique_users = cluster_data['user_id'].unique()\n",
    "    train_users = np.random.choice(unique_users, round(len(unique_users)*train_rate), False).tolist()\n",
    "    test_users = [user for user in unique_users if user not in train_users]\n",
    "    return train_users, test_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_tuples = [split_users_in_cluster(cluster, 0.75) for cluster in clusters_separated]\n",
    "train_users = [users[0] for users in train_test_tuples]\n",
    "test_users = [users[1] for users in train_test_tuples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational ease, we are going to save the test users in a separate directory to save time when testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_users_in_cluster(test_users: list, cluster_num: int):\n",
    "    with open(f'test_users_cluster{cluster_num}.pkl', 'wb') as file:\n",
    "        pickle.dump(test_users, file)\n",
    "    return f\"Test users for cluster {cluster_num} saved.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test users for cluster 1 saved.',\n",
       " 'Test users for cluster 2 saved.',\n",
       " 'Test users for cluster 3 saved.',\n",
       " 'Test users for cluster 4 saved.',\n",
       " 'Test users for cluster 5 saved.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[save_test_users_in_cluster(test_users[i], i+1) for i in range(len(test_users))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_product_lookup(products: pd.DataFrame):\n",
    "    product_lookup = dict(zip(products['product_id'].astype(str).to_list(), products['product_name'].to_list()))\n",
    "    with open('product_lookup.pkl', 'wb') as file:\n",
    "        pickle.dump(product_lookup, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_product_lookup(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_cluster(cluster: pd.DataFrame, train_users):\n",
    "    return cluster[cluster['user_id'].isin(train_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orders_from_cluster(cluster_subset):\n",
    "    return cluster_subset.groupby(['user_id', 'order_id'])['product_id'].apply(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_purchase_history_in_cluster(cluster: pd.DataFrame, train_users):\n",
    "    cluster_subset = subset_cluster(cluster, train_users)\n",
    "    purchase_history = get_orders_from_cluster(cluster_subset)\n",
    "    filtered_purchase_history = [purchase for purchase in purchase_history if len(purchase) > 3] # A number of purchases \n",
    "    return purchase_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_history_in_cluster = [generate_user_purchase_history_in_cluster(clusters_separated[i], train_users[i]) for i in range(0, len(clusters_separated))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item2vec_model(purchases_data):\n",
    "    model = Word2Vec(window=3, sg=1, hs=0, vector_size=100, negative=10, alpha=0.03, min_alpha=0.0007, seed=28101997, workers=6)\n",
    "    model.build_vocab(purchases_data, progress_per=200)\n",
    "    model.train(purchases_data, total_examples = model.corpus_count, epochs=10, report_delay=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [build_item2vec_model(purchase_history) for purchase_history in purchase_history_in_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cluster_model(model, id: int):\n",
    "    model.save(f'model_cluster_{id}.model')\n",
    "    return f\"Model for cluster {id} successfully saved.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model for cluster 0 successfully saved.',\n",
       " 'Model for cluster 1 successfully saved.',\n",
       " 'Model for cluster 2 successfully saved.',\n",
       " 'Model for cluster 3 successfully saved.',\n",
       " 'Model for cluster 4 successfully saved.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[save_cluster_model(models[i], i) for i in range(len(models))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
